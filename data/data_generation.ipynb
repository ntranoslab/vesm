{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook present the script to generate the ESMIN data from LLRs of ESM models.\n",
    "Here, we take the minimum of LLRS for 11 models below.\n",
    "The calculation of LLRs for each individual model can be performed with ```infer_llrs.py``. Please refer to the README.md for the running commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual llrs are stored in the ```base_llrs``` folder (Download the precomputed llrs [here](https://huggingface.co/datasets/ntranoslab/vesm_datasets/blob/main/base_llrs.zip)). One can generate different settings of ESMIN varying the set of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_name_dct = {\n",
    "     'esm1b': 'esm1b_t33_650M_UR50S',\n",
    "     'esm1v_1': 'esm1v_t33_650M_UR90S_1',\n",
    "     'esm1v_2': 'esm1v_t33_650M_UR90S_2',\n",
    "     'esm1v_3': 'esm1v_t33_650M_UR90S_3',\n",
    "     'esm1v_4': 'esm1v_t33_650M_UR90S_4',\n",
    "     'esm1v_5': 'esm1v_t33_650M_UR90S_5',\n",
    "     'esm2_650m': 'esm2_t33_650M_UR50D',\n",
    "     'esm2_3b': 'esm2_t36_3B_UR50D',\n",
    "     'esm2_150m': 'esm2_t30_150M_UR50D',\n",
    "     'esm2_8m': 'esm2_t6_8M_UR50D',\n",
    "     'esm2_35m': 'esm2_t12_35M_UR50D',\n",
    "}\n",
    "models = list(esm_name_dct.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(fpath):\n",
    "    with open(fpath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def read_pkl(fpath):\n",
    "    with open(fpath, 'rb') as fp:\n",
    "        dct = pickle.load(fp)\n",
    "    return dct\n",
    "\n",
    "def dict2json(json_pth, dct):\n",
    "    with open(json_pth, \"w\") as outfile:\n",
    "        json.dump(dct, outfile, indent=2)\n",
    "\n",
    "def try_makedir(fdir):\n",
    "    if not os.path.exists(fdir):\n",
    "        os.makedirs(fdir)\n",
    "\n",
    "def write_pkl(dct, fpath):\n",
    "    with open(fpath, 'wb') as fp:\n",
    "        pickle.dump(dct, fp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break long sequences into multiple segments\n",
    "def break_long_sequence(seq_length, model_window=1022):\n",
    "    half_window = model_window // 2\n",
    "    if seq_length <= model_window:\n",
    "        return [[0, seq_length]]\n",
    "    else:\n",
    "        lst = []\n",
    "        s = 0; e = 0\n",
    "        while e < seq_length:\n",
    "            e = min(seq_length, s + model_window)\n",
    "            lst.append([s, e])\n",
    "            s += half_window\n",
    "        return lst\n",
    "    \n",
    "def filter_segments(uid, sequence, scores, segments, max_len=1022):\n",
    "    indices = break_long_sequence(len(sequence), model_window=max_len)\n",
    "    data = {}\n",
    "    for k, (s, e) in enumerate(indices):\n",
    "        if s in segments:\n",
    "            data[f\"{uid}_segment_{s}\"] = {\n",
    "                \"llrs\": scores[s:e, :],\n",
    "                \"sequence\": sequence[s:e]\n",
    "            }\n",
    "    return data\n",
    "\n",
    "def get_stats(data_dct, key=None):\n",
    "    n = 0; mu = 0; ss = 0\n",
    "    min_e = 1000; max_e = -1000\n",
    "    for _, arr in data_dct.items():\n",
    "        if key == 'llrs':\n",
    "            arr = arr['llrs']\n",
    "        elif key == 'shorten':\n",
    "            arr = arr[1:-1, 4:24]\n",
    "        size_protein = arr.shape[0] * arr.shape[1]\n",
    "        mu_protein = arr.mean()\n",
    "        std_protein = arr.std()\n",
    "        n += size_protein\n",
    "        mu += mu_protein * size_protein\n",
    "        ss_protein = (std_protein**2 + mu_protein**2) * size_protein\n",
    "        ss += ss_protein\n",
    "    mu = mu / n\n",
    "    variance = ss/n - mu**2\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data from base model llrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \".\" # data folder\n",
    "llr_dir = f\"{data_dir}/base_llrs\" # folder contains llrs for all individual models\n",
    "\n",
    "meta_dct = read_pkl(f\"{data_dir}/train/UniProtKB_meta_data.pkl\") \n",
    "segment_dct = meta_dct[\"segment_dct\"]\n",
    "seq_dct = meta_dct[\"seq_dct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# collect all scores \n",
    "esm_llr_dict = {m: read_pkl(f\"{llr_dir}/llrs_{m}.pkl\") for m in models}\n",
    "esmin_dct = {}\n",
    "\n",
    "# processing short sequences\n",
    "for protein in  meta_dct[\"short_proteins\"]:\n",
    "    llrs_lst = [esm_llr_dict[m][protein] for m in models]\n",
    "    esmin_dct[protein] = {\n",
    "        \"sequence\": seq_dct[protein],\n",
    "        \"llrs\": np.minimum.reduce(llrs_lst)\n",
    "    }\n",
    "\n",
    "# breaking long sequences\n",
    "for protein in meta_dct[\"long_proteins\"]:\n",
    "    llrs_lst = [esm_llr_dict[m][protein] for m in models]\n",
    "    scores = np.minimum.reduce(llrs_lst)\n",
    "    segs = filter_segments(protein, seq_dct[protein], scores, segment_dct[protein], max_len=1022)\n",
    "    esmin_dct.update(segs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the statitstics for adaptive mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"min\": -10.223638271060164,\n",
      "\"esm1b\": -8.279172428875482,\n",
      "\"esm1v_1\": -6.651073636222643,\n",
      "\"esm1v_2\": -7.217647505916745,\n",
      "\"esm1v_3\": -6.587440777276117,\n",
      "\"esm1v_4\": -6.823804579465309,\n",
      "\"esm1v_5\": -7.280105659452346,\n",
      "\"esm2_650m\": -6.9738925928732,\n",
      "\"esm2_3b\": -8.36833643554398,\n",
      "\"esm2_150m\": -5.512521036469033,\n",
      "\"esm2_8m\": -4.265973882034055,\n",
      "\"esm2_35m\": -4.763005785707714,\n"
     ]
    }
   ],
   "source": [
    "# Statistics \n",
    "print(f'\"min\": {get_stats(esmin_dct, \"llrs\")},')\n",
    "for m in models:\n",
    "    stats = get_stats(esm_llr_dict[m])\n",
    "    print(f'\"{m}\": {stats},')\n",
    "    \n",
    "write_pkl(esmin_dct, f\"{data_dir}/train/ESM11_UniProt_min.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protssn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
