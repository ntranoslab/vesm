{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDnhlGjG9nlp"
      },
      "source": [
        "# VESM: Getting Started (Quickstart & Inference)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ntranoslab/vesm/blob/main/notebooks/VESM_Getting_Started.ipynb)\n",
        "\n",
        "### Requirements\n",
        "- Python 3.9+ (recommended), `torch`, `transformers`, `huggingface_hub`, `numpy`, `matplotlib`, `seaborn`\n",
        "- GPU is optional but recommended for speed\n",
        "\n",
        "## Table of contents\n",
        "1. [Setup & Imports](#setup--imports)\n",
        "2. [Load a VESM Model](#load-a-model)\n",
        "3. [Run Inference & Get Scores](#vesm_inference)\n",
        "4. [Visualize Results](#visualize-results)  \n",
        "5. [Download Prediction Scores](#download-prediction-scores)\n",
        "6. [VESM3 Inference](#vesm3-inference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StxqMisv9nlq"
      },
      "source": [
        "<a id=\"setup--imports\"></a>\n",
        "\n",
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyJ2t9r_9nlr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, EsmForMaskedLM\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", FutureWarning) # ignore future warnings from transformers package\n",
        "warnings.simplefilter(\"ignore\", UserWarning) # ignore user warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVDZWroepHeo"
      },
      "outputs": [],
      "source": [
        "local_dir = 'vesm' # local directory to store models\n",
        "\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "    device = torch.device('cuda:0')\n",
        "else:\n",
        "    device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-kj1lMlZ__2"
      },
      "source": [
        "<a id=\"load-a-model\"></a>\n",
        "\n",
        "## 2. Load VESM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVaJcWoG9nls"
      },
      "outputs": [],
      "source": [
        "esm_dict = {\n",
        "    \"VESM_3B\": 'facebook/esm2_t36_3B_UR50D',\n",
        "    \"VESM_650M\": 'facebook/esm2_t33_650M_UR50D',\n",
        "    \"VESM_150M\": 'facebook/esm2_t30_150M_UR50D',\n",
        "    \"VESM_35M\": 'facebook/esm2_t12_35M_UR50D',\n",
        "    \"VESM3\": \"esm3_sm_open_v1\"\n",
        "}\n",
        "\n",
        "def load_vesm(model_name=\"VESM_3B\", local_dir=\"vesm\", device='cuda'):\n",
        "    if model_name in esm_dict:\n",
        "        ckt = esm_dict[model_name]\n",
        "    else:\n",
        "        print(\"Model not found\")\n",
        "        return None\n",
        "    # download weights\n",
        "    hf_hub_download(repo_id=\"ntranoslab/vesm\", filename=f\"{model_name}.pth\", local_dir=local_dir)\n",
        "    # load base model\n",
        "    if model_name == \"VESM3\":\n",
        "      from esm.models.esm3 import ESM3\n",
        "      model = ESM3.from_pretrained(ckt, device=device).to(torch.float)\n",
        "      tokenizer = model.tokenizers.sequence\n",
        "    else:\n",
        "      model = EsmForMaskedLM.from_pretrained(ckt).to(device)\n",
        "      tokenizer = AutoTokenizer.from_pretrained(ckt)\n",
        "    # load pretrained VESM\n",
        "    model.load_state_dict(torch.load(f'{local_dir}/{model_name}.pth'), strict=False)\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIS6DPkh9nls"
      },
      "source": [
        "We first load the VESM_3B checkpoint from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zNvkI79pHZN"
      },
      "outputs": [],
      "source": [
        "model_name = 'VESM_3B'\n",
        "model, tokenizer = load_vesm(model_name, local_dir=local_dir, device=device)\n",
        "sequence_vocabs = tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "360gc-PsvSzC"
      },
      "source": [
        "# VESM Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmgwuFYh9nls"
      },
      "source": [
        "The following function is to get log-likelihood ratio (LLR) scores for all possible single missense mutations of a given sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IAMr_ISa5-y"
      },
      "outputs": [],
      "source": [
        "def get_LLR(sequence, esm_model, device='cuda'):\n",
        "  \"\"\"\n",
        "    @param sequence: str, input protein sequence\n",
        "    @param esm_model: loaded VESM model\n",
        "    @param device: 'cuda' or 'cpu'\n",
        "    @return: pd.DataFrame, LLR scores for all missense mutations\n",
        "  \"\"\"\n",
        "  tokens = tokenizer(sequence, return_tensors='pt')\n",
        "  batch_tokens = tokens['input_ids']\n",
        "  esm_model.eval()\n",
        "  with torch.no_grad():\n",
        "    logits =torch.log_softmax(esm_model(batch_tokens.to(device),)['logits'],dim=-1)[0,:,:].cpu()\n",
        "  tok = batch_tokens[0,:].cpu()\n",
        "  wt_norm = logits[np.arange(len(tok)), tok].unsqueeze(1)\n",
        "  LLR = logits - wt_norm\n",
        "  LLR=LLR[1:-1,4:24].numpy()\n",
        "  AAorder=['K','R','H','E','D','N','Q','T','S','C','G','A','V','L','I','M','P','Y','F','W']\n",
        "  LLR_ = pd.DataFrame(LLR,columns=tokenizer.all_tokens[4:24],index=list(sequence)).T.loc[AAorder]\n",
        "  LLR_.columns = [j.split('.')[0]+' '+str(i+1) for i,j in enumerate(LLR_.columns)]\n",
        "  return LLR_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ranELa9M9nlt"
      },
      "source": [
        "<a id=\"vesm_inference\"></a>\n",
        "## 3. Inference on a protein sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCtEWt9e-j--"
      },
      "outputs": [],
      "source": [
        "protein_sequence = 'MVTLGVISLLENILVIVAIAKNKLHSPMYFFICSLAVADMLVSVSNGSET'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "776LcEq4-f17"
      },
      "source": [
        "compute log-likelihood ratio scores (LLR) for all missense mutations with VESM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2orm8lfhfs2h"
      },
      "outputs": [],
      "source": [
        "get_LLR(protein_sequence, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U13lyiY_bNLL"
      },
      "source": [
        "<a id=\"visualize-results\"></a>\n",
        "\n",
        "## 4. Visualizing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FfBtYd2acxR"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from google.colab import output\n",
        "\n",
        "AAorder=['K','R','H','E','D','N','Q','T','S','C','G','A','V','L','I','M','P','Y','F','W']\n",
        "\n",
        "def plot_interactive(LLR,higher_than_wt=False,thresh=2,zmax=0,cmap='Viridis_r'):\n",
        "\n",
        "  TITLE=''\n",
        "\n",
        "  is_dark = output.eval_js('document.documentElement.matches(\"[theme=dark]\")')\n",
        "  template='plotly_dark' if is_dark else 'plotly_white'\n",
        "\n",
        "  fig = px.imshow(LLR.values, x=LLR.columns, y=LLR.index, color_continuous_scale=cmap,zmax=zmax,\n",
        "                  labels=dict(y=\"Amino acid change\", x=\"Protein sequence\", color=\"LLR\"),\n",
        "                  template=template,\n",
        "                  title=TITLE)\n",
        "  fig.update_xaxes(tickangle=-90,range=[0,99],rangeslider=dict(visible=True),dtick=1)\n",
        "  fig.update_yaxes(dtick=1)\n",
        "  fig.update_layout({\n",
        "  'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n",
        "  'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n",
        "  },font={'family':'Arial','size':11},\n",
        "  hoverlabel=dict(font=dict(family='Arial', size=14)))\n",
        "\n",
        "  fig.update_traces(\n",
        "      hovertemplate=\"<br>\".join([\n",
        "          \"<b>%{x} %{y}</b>\"+\n",
        "          \" (%{z:.2f})\",\n",
        "      ])+'<extra></extra>'\n",
        "  )\n",
        "  if higher_than_wt:\n",
        "    hwt_x=[]\n",
        "    hwt_y=[]\n",
        "    cust=[]\n",
        "    for i in LLR.columns:\n",
        "      for j in list(LLR.index[LLR[i]>thresh]):\n",
        "        hwt_x+=[i]\n",
        "        hwt_y+=[j]\n",
        "        cust+=[LLR.loc[j,i]]\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=hwt_x,\n",
        "        y=hwt_y,\n",
        "        customdata=cust,\n",
        "        mode='markers',\n",
        "        marker=dict(size=8),\n",
        "        hovertemplate=\"<br>\".join([\n",
        "            \"<b>%{x} %{y}</b>\"+\n",
        "            \" (%{customdata:.2f})\",\n",
        "        ])+'<extra></extra>')\n",
        "    )\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "def plot_heatmap(LLR,figname=None,vmin=None):\n",
        "  \"\"\"\n",
        "    Plot a per-residue score heatmap.\n",
        "      Args:\n",
        "          scores: 2D array-like of shape (positions, alphabet) or 1D positional scores.\n",
        "          figname: Optional basename to save PNG.\n",
        "          range: Color scale range for heatmap.\n",
        "  \"\"\"\n",
        "  primaryLLR=LLR\n",
        "  plt.figure(figsize=(int(np.round(primaryLLR.shape[1]*90/390)),5))\n",
        "  sns.heatmap( primaryLLR ,cmap='viridis_r',xticklabels=True, yticklabels=True,vmax=0,vmin=vmin)\n",
        "  if not figname is None:\n",
        "    plt.savefig(f\"{figname}.png\", dpi=300,bbox_inches = 'tight')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuJAdZYc9nlt"
      },
      "source": [
        "### Visualizing LLRs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swh8ssB4acuq"
      },
      "outputs": [],
      "source": [
        "# MC4R\n",
        "sequence = 'MVNSTHRGMHTSLHLWNRSSYRLHSNASESLGKGYSDGGCYEQLFVSPEVFVTLGVISLLENILVIVAIAKNKNLHSPMYFFICSLAVADMLVSVSNGSETIVITLLNSTDTDAQSFTVNIDNVIDSVICSSLLASICSLLSIAVDRYFTIFYALQYHNIMTVKRVGIIISCIWAACTVSGILFIIYSDSSAVIICLITMFFTMLALMASLYVHMFLMARLHIKRIAVLPGTGAIRQGANMKGAITLTILIGVFVVCWAPFFLHLIFYISCPQNPYCVCFMSHFNLYLILIMCNSIIDPLIYALRSQELRKTFKEIICCYPLGGLCDLSSRY'\n",
        "plot_interactive(get_LLR(sequence, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oITokwqk14k3"
      },
      "source": [
        "<a id=\"visualize_structure\"></a>\n",
        "\n",
        "### Visualize on Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7ttwf6334cS"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet py3Dmol biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAxIOsp51mGm"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import py3Dmol\n",
        "from Bio.SeqUtils import seq1\n",
        "from Bio.PDB import PDBParser\n",
        "\n",
        "def show_pdb(position_scores, pdb_file, width=800, height=600, surface=False):\n",
        "    view = py3Dmol.view(width=width, height=height, js='https://3dmol.org/build/3Dmol.js')\n",
        "    view.addModel(open(pdb_file, 'r').read(), 'pdb')\n",
        "    sigmoid_scores = 1 / (1 + np.exp((0.6 * np.array(position_scores) + 6)))\n",
        "    for i, value in enumerate(sigmoid_scores):\n",
        "        rgba = plt.cm.coolwarm(value)\n",
        "        hexcol = matplotlib.colors.rgb2hex(rgba[:3])\n",
        "        view.setStyle({'resi': str(i+1)}, {'cartoon': {'color': hexcol}})\n",
        "    view.setBackgroundColor('#383838')\n",
        "\n",
        "    if surface:\n",
        "        for i, value in enumerate(sigmoid_scores):\n",
        "            rgba = plt.cm.coolwarm(value)\n",
        "            hexcol = matplotlib.colors.rgb2hex(rgba[:3])\n",
        "            view.addSurface(\n",
        "                py3Dmol.VDW,\n",
        "                { 'opacity': 0.7, 'color': hexcol },\n",
        "                { 'resi': str(i+1) }\n",
        "            )\n",
        "        view.setBackgroundColor('white')\n",
        "\n",
        "\n",
        "    view.zoomTo()\n",
        "    return view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoMBCLfuuled"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "def download_latest_af_pdb(uniprot_id, version_start=20, version_end=1, out_name=None):\n",
        "    \"\"\"\n",
        "    Download the latest AF PDB model version for a UniProt ID.\n",
        "\n",
        "    Args:\n",
        "        uniprot_id (str): UniProt accession, e.g., \"P12345\"\n",
        "        version_start (int): Highest version to check, e.g., 10\n",
        "        version_end (int): Lowest version to check, e.g., 1\n",
        "        out_name (str): Optional output filename. If None, use the server's name.\n",
        "    \"\"\"\n",
        "    base_url = \"https://alphafold.ebi.ac.uk/files\"\n",
        "\n",
        "    for v in range(version_start, version_end - 1, -1):\n",
        "        url = f\"{base_url}/AF-{uniprot_id}-F1-model_v{v}.pdb\"\n",
        "        head = requests.head(url)\n",
        "\n",
        "        if head.status_code == 200:\n",
        "            print(f\"Found version v{v}: {url}\")\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Determine filename\n",
        "            filename = out_name or url.split(\"/\")[-1]\n",
        "            with open(filename, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Downloaded to: {filename}\")\n",
        "            return filename\n",
        "\n",
        "    print(f\"No model found for UniProt ID '{uniprot_id}' in versions v{version_start}–v{version_end}.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c1ZJNY7-xuZ"
      },
      "source": [
        "Example structure from AFDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPkMguahoekV"
      },
      "outputs": [],
      "source": [
        "uniprot_id = 'P32245'\n",
        "pdb_file = download_latest_af_pdb(uniprot_id, version_start=10, version_end=4, out_name=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klCEavbR-0yx"
      },
      "source": [
        "extract sequence from pdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3zZ85SY-3RN"
      },
      "outputs": [],
      "source": [
        "sequence = \"\".join(\n",
        "    seq1(res.get_resname())\n",
        "    for res in PDBParser(QUIET=True).get_structure(\"model\", pdb_file).get_residues()\n",
        "    if res.id[0] == \" \"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozqk9k4J-6c3"
      },
      "source": [
        "average scores per position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIZsnFSu1lzD"
      },
      "outputs": [],
      "source": [
        "position_scores = get_LLR(sequence, model).values.mean(0)\n",
        "show_pdb(position_scores,pdb_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1mFsX_TeZd5"
      },
      "source": [
        "<a id=\"download\"></a>\n",
        "## Download Prediction Scores\n",
        "\n",
        "- **sequence**: input any amino acid sequence for LLR inference.\n",
        "- **seq_name**: name the sequence for the file name (optional).\n",
        "- **heatmap**: check if you want to download the LLR heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CmGnF2wJjEtn"
      },
      "outputs": [],
      "source": [
        "sequence = 'MVNSTHRGMHTSLHLWNRSSYRLHSNASESLGKGYSDGGCYEQLFVSPEVFVTLGVISLLENILVIVAIAKNKNLHSPMYFFICSLAVADMLVSVSNGSETIVITLLNSTDTDAQSFTVNIDNVIDSVICSSLLASICSLLSIAVDRYFTIFYALQYHNIMTVKRVGIIISCIWAACTVSGILFIIYSDSSAVIICLITMFFTMLALMASLYVHMFLMARLHIKRIAVLPGTGAIRQGANMKGAITLTILIGVFVVCWAPFFLHLIFYISCPQNPYCVCFMSHFNLYLILIMCNSIIDPLIYALRSQELRKTFKEIICCYPLGGLCDLSSRY' #@param {type:\"string\"}\n",
        "seq_name = \"\" #@param {type:\"string\"}\n",
        "heatmap = True #@param {type:\"boolean\"}\n",
        "\n",
        "import hashlib, zipfile\n",
        "from google.colab import files\n",
        "\n",
        "def short_hash(seq, length=8):\n",
        "    return hashlib.sha1(seq.encode()).hexdigest()[:length]\n",
        "\n",
        "def meltLLR(LLR,gene_prefix=None,ignore_pos=False):\n",
        "  vars = LLR.melt(ignore_index=False)\n",
        "  vars['variant'] = [''.join(i.split(' '))+j for i,j in zip(vars['variable'],vars.index)]\n",
        "  vars['score'] = vars['value']\n",
        "  vars = vars.set_index('variant')\n",
        "  if not ignore_pos:\n",
        "    vars['pos'] = [int(i[1:-1]) for i in vars.index]\n",
        "  del vars['variable'],vars['value']\n",
        "  if gene_prefix is not None:\n",
        "    vars.index=gene_prefix+'_'+vars.index\n",
        "  return vars\n",
        "\n",
        "base_name = f'vesm_LLR_{seq_name if len(seq_name) > 0 else short_hash(sequence)}'\n",
        "csv_file = f\"{base_name}.csv\"\n",
        "LLR_scores = get_LLR(sequence, model)\n",
        "\n",
        "# save csv file\n",
        "meltLLR(LLR_scores).to_csv(csv_file)\n",
        "\n",
        "if heatmap:\n",
        "    png_file = f\"{base_name}.png\"\n",
        "    plot_heatmap(LLR_scores, figname=base_name, vmin=None)\n",
        "\n",
        "    # Create results zip with both files\n",
        "    zip_name = f\"{base_name}_results.zip\"\n",
        "    with zipfile.ZipFile(zip_name, 'w') as zipf:\n",
        "        zipf.write(csv_file)\n",
        "        zipf.write(png_file)\n",
        "\n",
        "file_name = zip_name if heatmap else csv_file\n",
        "\n",
        "print(f\"Saved results to: {file_name}\")\n",
        "files.download(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVk_QQTPvdeb"
      },
      "source": [
        "<a id=\"VESM3\"></a>\n",
        "\n",
        "# VESM3 Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJS_FaAU9nlu"
      },
      "source": [
        "Downloading the base ESM3-open model requires huggingface login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDW5C6l2zjIE"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx8dkQ8hsW_q"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet esm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7npRp-gi9nlu"
      },
      "source": [
        "remove previous model (necessary if using Colab's T4 GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w3MfDMmmSWN"
      },
      "outputs": [],
      "source": [
        "if 'model' in globals():\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    import gc; gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YICz5HKW9nlu"
      },
      "source": [
        "Load VESM3 checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhbfyCyfiBMl"
      },
      "outputs": [],
      "source": [
        "vesm3, _ = load_vesm(\"VESM3\", local_dir=local_dir, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCFS0xpKt4FO"
      },
      "outputs": [],
      "source": [
        "def get_vesm3_LLR_sequence(sequence, esm3_model):\n",
        "  from esm.sdk.api import ESMProtein\n",
        "  protein = ESMProtein(sequence=sequence)\n",
        "  tokens = esm3_model.encode(protein).sequence\n",
        "  with torch.no_grad():\n",
        "    logits =torch.log_softmax(esm3_model.forward(sequence_tokens=tokens.reshape(1,-1)).sequence_logits[0, :, :], dim=-1).cpu()\n",
        "  tok = tokens.cpu()\n",
        "  wt_norm = logits[np.arange(len(tok)), tok].unsqueeze(1)\n",
        "  LLR = logits - wt_norm\n",
        "\n",
        "  AAorder=['K','R','H','E','D','N','Q','T','S','C','G','A','V','L','I','M','P','Y','F','W']\n",
        "  order = [esm3_model.tokenizers.sequence.vocab[x] for x in AAorder]\n",
        "  LLR=LLR[1:-1,order].numpy()\n",
        "  LLR_ = pd.DataFrame(LLR,columns=AAorder,index=list(sequence)).T\n",
        "  LLR_.columns = [j.split('.')[0]+' '+str(i+1) for i,j in enumerate(LLR_.columns)]\n",
        "  return LLR_\n",
        "\n",
        "\n",
        "def get_vesm3_LLR_structure(pdb_file, esm3_model):\n",
        "  from esm.sdk.api import ESMProtein\n",
        "  protein = ESMProtein.from_pdb(pdb_file)\n",
        "  tokens = esm3_model.encode(protein)\n",
        "  seq_tokens = tokens.sequence\n",
        "  struct_tokens = tokens.structure\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits =torch.log_softmax(esm3_model.forward(sequence_tokens=seq_tokens.reshape(1,-1), structure_tokens=struct_tokens.reshape(1,-1) ).sequence_logits[0, :, :], dim=-1).cpu()\n",
        "  tok = seq_tokens.cpu()\n",
        "  wt_norm = logits[np.arange(len(tok)), tok].unsqueeze(1)\n",
        "  LLR = logits - wt_norm\n",
        "\n",
        "  AAorder=['K','R','H','E','D','N','Q','T','S','C','G','A','V','L','I','M','P','Y','F','W']\n",
        "  order = [esm3_model.tokenizers.sequence.vocab[x] for x in AAorder]\n",
        "  LLR=LLR[1:-1,order].numpy()\n",
        "  LLR_ = pd.DataFrame(LLR,columns=AAorder,index=list(sequence)).T\n",
        "  LLR_.columns = [j.split('.')[0]+' '+str(i+1) for i,j in enumerate(LLR_.columns)]\n",
        "  return LLR_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBTIkGjv9nlu"
      },
      "outputs": [],
      "source": [
        "### Example structure from AFDB\n",
        "uniprot_id = 'P32245'\n",
        "pdb_file = download_latest_af_pdb(uniprot_id, version_start=10, version_end=1, out_name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqq0vskjZ46O"
      },
      "outputs": [],
      "source": [
        "from Bio.PDB import PDBParser\n",
        "from Bio.SeqUtils import seq1\n",
        "# extract sequence from pdb\n",
        "sequence = \"\".join(\n",
        "    seq1(res.get_resname())\n",
        "    for res in PDBParser(QUIET=True).get_structure(\"model\", pdb_file).get_residues()\n",
        "    if res.id[0] == \" \"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b5Sfmhd9nlv"
      },
      "source": [
        "### Inference with sequence only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo2suGj5v-vc"
      },
      "outputs": [],
      "source": [
        "LLR_seq = get_vesm3_LLR_sequence(sequence, vesm3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3--Qg9-A9nly"
      },
      "source": [
        "### Inference with structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn9hejLTv-sm"
      },
      "outputs": [],
      "source": [
        "LLR_struct = get_vesm3_LLR_structure(pdb_file, vesm3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ8dOgXE_-cI"
      },
      "source": [
        "### Visualize Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G7rZrGD9nly"
      },
      "source": [
        "Visualize differences between sequence- and structure- derived LLRs with VESM3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR3rs4tJv-mn"
      },
      "outputs": [],
      "source": [
        "# LLR_seq vs LLR_struct\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(LLR_seq.values.reshape(-1), LLR_struct.values.reshape(-1), '.')\n",
        "plt.plot([-20, 5], [-20, 5], 'k--')\n",
        "plt.xlabel('LLR from Sequence')\n",
        "plt.ylabel('LLR from Structure')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "famkD3ZQ9nly"
      },
      "source": [
        "Plot the heatmap of differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACfccGb701UC"
      },
      "outputs": [],
      "source": [
        "def plot_diff_heatmap(diff,figname=None,range=4):\n",
        "  plt.figure(figsize=(int(np.round(diff.shape[1]*90/390)),5))\n",
        "  sns.heatmap( diff ,cmap='coolwarm_r',xticklabels=True, yticklabels=True,vmax=range,vmin=-range)\n",
        "  if not figname is None:\n",
        "    plt.savefig(f\"{figname}.png\", dpi=300,bbox_inches = 'tight')\n",
        "  plt.show()\n",
        "\n",
        "diff = LLR_struct - LLR_seq\n",
        "plot_heatmap(LLR_seq)\n",
        "threshold = -5\n",
        "plot_diff_heatmap(diff[((LLR_struct<threshold) & (LLR_seq>threshold)) | ((LLR_struct>threshold) & (LLR_seq<threshold))])\n",
        "plot_heatmap(LLR_struct)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYFfY0qogtmK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "py312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}